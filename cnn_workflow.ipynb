{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## settings\n",
    "\n",
    "dinp = \"D:\\\\uni\\\\deep\" # dir input\n",
    "dpat = os.path.join(dinp, \"img_patches\") # output dir of img patch\n",
    "imgn = os.path.join(dinp, \"image.tif\") # image name\n",
    "shpn = os.path.join(dinp, \"hymap_lcz_test.shp\") # shp name\n",
    "attr = \"classNum\" # col name of class labels in shapefile\n",
    "tbdir  = \"C:\\\\Users\\\\Gekko\\\\Anaconda3\\\\Scripts\\\\tensorboard\" # dir tb\n",
    "modelpath = os.path.join(dinp, \"tf_model\", \"TFmodel.ckpt\")\n",
    "target = os.path.join(dinp, \"subset99.tif\")\n",
    "\n",
    "patchesPerClass = 7\n",
    "patchSize = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "dropout = 0.75\n",
    "display_step = 5\n",
    "num_ch = 5\n",
    "num_classes = 8\n",
    "traintestsplit = 0.8\n",
    "batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## start tensorboard\n",
    "\n",
    "tblog = os.path.join(dinp, \"tb_logs\") # dir logs\n",
    "tblogtrain = os.path.join(tblog, \"train\") # logs train\n",
    "tblogtest = os.path.join(tblog, \"test\") # logs test\n",
    "subprocess.Popen([tbdir, \"--logdir=testor:\" + tblog, \"--port=6006\"])\n",
    "os.startfile(\"http://homer:6006/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## produce image patches\n",
    "\n",
    "# create outputfolder\n",
    "if not os.path.exists(dpat):\n",
    "    os.makedirs(dpat)\n",
    "\n",
    "# import image\n",
    "img = gdal.Open(imgn)\n",
    "\n",
    "# rasterize shapefile\n",
    "shp = ogr.Open(shpn)\n",
    "shpl = shp.GetLayer()\n",
    "rs = gdal.GetDriverByName('GTiff')\n",
    "shpr = shpn[:-4]+\".tif\"\n",
    "rs = rs.Create(shpr, img.RasterXSize, img.RasterYSize, 1, gdal.GDT_Byte)\n",
    "rs.SetGeoTransform(img.GetGeoTransform())\n",
    "rs.SetProjection(img.GetProjectionRef())\n",
    "gdal.RasterizeLayer(rs, [1], shpl, options=[str(\"ATTRIBUTE=\" + attr)])\n",
    "rs = None\n",
    "\n",
    "# import rasterized shapefile\n",
    "spl = gdal.Open(shpr)\n",
    "splval= spl.ReadAsArray()\n",
    "\n",
    "# create image patches\n",
    "for cla in np.unique(splval):\n",
    "    if cla > 0 and cla < 99:\n",
    "        xy = np.argwhere(splval == cla)\n",
    "        xy = xy[np.random.randint(xy.shape[0], size=patchesPerClass), :]\n",
    "        for idx, loc in enumerate(xy):            \n",
    "            out = os.path.join(dpat, str(cla) + \"_\" + str(idx) + \".tif\")            \n",
    "            gdal.Translate(out, imgn, format = 'GTiff', srcWin = [loc[1]-patchSize/2,\n",
    "                                                                 loc[0]-patchSize/2,\n",
    "                                                                 patchSize,\n",
    "                                                                 patchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image patches\n",
    "\n",
    "# load as list of nparrays\n",
    "patches = os.listdir(dpat)\n",
    "patchesX = []\n",
    "patchesY = []\n",
    "\n",
    "for idx, f in enumerate(patches):\n",
    "    pclass = int(f.split(\"_\")[0])\n",
    "    labtmp = np.zeros((num_classes))\n",
    "    labtmp[pclass-1] = 1\n",
    "    patchesY.append(labtmp)\n",
    "    \n",
    "    patchval = gdal.Open(os.path.join(dpat, f)).ReadAsArray()\n",
    "    patchval = np.moveaxis(patchval, 0, -1)\n",
    "    patchesX.append(patchval)\n",
    "\n",
    "# normalize\n",
    "means = []\n",
    "stds = []\n",
    "z = np.array(patchesX, dtype=np.float32)\n",
    "\n",
    "for i in range(z.shape[3]):\n",
    "    mean = np.mean(z[:, :, :, i])\n",
    "    std = np.std(z[:, :, :, i]) \n",
    "    z[:, :, :, i] = (z[:, :, :, i] - mean) / std\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "    \n",
    "patchesX = list(z)\n",
    "\n",
    "# shuffle\n",
    "order = np.random.permutation(len(patchesX))\n",
    "patchesX = [patchesX[i] for i in order]\n",
    "patchesY = [patchesY[i] for i in order]\n",
    "\n",
    "# train test split\n",
    "split = int(len(patchesX) * traintestsplit)\n",
    "X_train = patchesX[:split]\n",
    "Y_train = patchesY[:split]\n",
    "X_test = patchesX[split:]\n",
    "Y_test = patchesY[split:]\n",
    "\n",
    "# create train data batch intervals \n",
    "batches = []\n",
    "for i in range(0, len(X_train), batchsize):\n",
    "    batches.append(i)\n",
    "if not batches[-1] == len(X_train):\n",
    "    batches.append(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TF model setup\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, patchSize, patchSize, num_ch])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, num_ch, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4096, 128])),\n",
    "    'out': tf.Variable(tf.random_normal([128, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([128])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorboard\n",
    "tf.summary.image('input_x', X[:12, :, :, :3], 12) \n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "merge = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(tblogtrain)\n",
    "test_writer = tf.summary.FileWriter(tblogtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TF training\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(1, epochs+1): \n",
    "        \n",
    "        for b in range(len(batches)-1):            \n",
    "            sess.run(train_op, feed_dict={X: X_train[batches[b]:batches[b+1]], \n",
    "                                          Y: Y_train[batches[b]:batches[b+1]],  \n",
    "                                          keep_prob: dropout})\n",
    "\n",
    "        if epoch % display_step == 0 or epoch == 1:\n",
    "            summaryTr, lossTr, accTr = sess.run([merge, loss_op, accuracy],\n",
    "                                                feed_dict={X: X_train[batches[b]:batches[b+1]],\n",
    "                                                           Y: Y_train[batches[b]:batches[b+1]],\n",
    "                                                           keep_prob: 1.0})\n",
    "            \n",
    "            summaryTe, accTe = sess.run([merge, accuracy],\n",
    "                                        feed_dict={X: X_test, Y: Y_test, keep_prob: 1.0})\n",
    "\n",
    "            print(\"Epoch \" + str(epoch) + \"/\" + str(epochs) + \\\n",
    "                  \", MbLoss= \" + \"{:.3f}\".format(lossTr) + \\\n",
    "                  \", TrAcc= \" + \"{:.3f}\".format(accTr) + \\\n",
    "                  \", TeAcc= \" + \"{:.3f}\".format(accTe))\n",
    "\n",
    "            train_writer.add_summary(summaryTr, epoch)\n",
    "            test_writer.add_summary(summaryTe, epoch)\n",
    "\n",
    "            save_path = saver.save(sess, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## classification of image\n",
    "\n",
    "# import\n",
    "imgc = gdal.Open(target)\n",
    "img = imgc.ReadAsArray().astype(np.float32)\n",
    "\n",
    "# normalization\n",
    "for i in range(len(means)):\n",
    "    img[i, :, :] = (img[i, :, :] - means[i]) / stds[i]\n",
    "\n",
    "classification = np.zeros((img.shape[1], img.shape[2]))\n",
    "Ypatch = np.zeros((1, num_classes))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, modelpath)\n",
    "    \n",
    "    for x in range(0, img.shape[1]-patchSize):\n",
    "        for y in range(0, img.shape[2]-patchSize):\n",
    "            Xpatch = img[:, x:(x+patchSize), y:(y+patchSize)]\n",
    "            Xpatch = np.moveaxis(Xpatch, 0, -1)\n",
    "            Xpatch = np.expand_dims(Xpatch, axis=0)\n",
    "            \n",
    "            pred = sess.run(prediction, feed_dict={X: Xpatch, \n",
    "                                                   Y: Ypatch, \n",
    "                                                   keep_prob:1.0})\n",
    "            classmax = np.argmax(pred)\n",
    "            classification[x+int(patchSize/2), y+int(patchSize/2)] = classmax\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20,10))\n",
    "imgplot = plt.imshow(classification)\n",
    "\n",
    "# export\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "df = driver.Create(target[:-4]+\"_deep.tif\", imgc.RasterXSize, imgc.RasterYSize, 1, gdal.GDT_Byte)\n",
    "df.SetGeoTransform(imgc.GetGeoTransform())\n",
    "df.SetProjection(imgc.GetProjectionRef())\n",
    "classification = classification.transpose()\n",
    "df.GetRasterBand(1).WriteArray(classification)\n",
    "df.FlushCache()\n",
    "df = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ten]",
   "language": "python",
   "name": "conda-env-ten-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
