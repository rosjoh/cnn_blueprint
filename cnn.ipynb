{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# inputs\n",
    "area = \"BER\"\n",
    "dinp = \"D:\\\\uni\\\\deep\" # dir input\n",
    "fimg = os.path.join(dinp, \"image.tif\") # image file\n",
    "fshp = os.path.join(dinp, \"hymap_lcz_test.shp\") # shp file\n",
    "attr = \"classNum\" # class attribute in shapefile\n",
    "dtbo = \"C:\\\\Users\\\\Gekko\\\\Anaconda3\\\\Scripts\\\\tensorboard\" # dir tb\n",
    "targ = os.path.join(dinp, \"subset2.tif\") # tmp: image subset for classification\n",
    "\n",
    "# outputs\n",
    "dptr = os.path.join(dinp, \"img_patches\", \"train\") # dir image patches train\n",
    "dpte = os.path.join(dinp, \"img_patches\", \"test\") # dir image patches test\n",
    "fmod = os.path.join(dinp, \"tf_model\", \"TFmodel.ckpt\") # output model\n",
    "dtbl = os.path.join(dinp, \"tb_logs\") # tensorboard dir logs\n",
    "\n",
    "# network settings\n",
    "sampleDistance = 6\n",
    "trainTestRatio = 0.8\n",
    "maxPatchesPerClass = 25\n",
    "patchSize = 32\n",
    "numChannels = 5\n",
    "numClasses = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "dropout = 0.75\n",
    "batchsize = 8\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TENSORBOARD\n",
    "\n",
    "subprocess.Popen([dtbo, \"--logdir=testor:\" + dtbl, \"--port=6006\"])\n",
    "os.startfile(str(\"http://\" + os.environ['COMPUTERNAME'] + \":6006/#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PRODUCE IMAGE PATCHES\n",
    "\n",
    "# create outputfolder\n",
    "if not os.path.exists(dptr):\n",
    "    os.makedirs(dptr)\n",
    "    os.makedirs(dpte)\n",
    "\n",
    "# import image\n",
    "img = gdal.Open(fimg)\n",
    "\n",
    "# rasterize shapefile\n",
    "shp = ogr.Open(fshp)\n",
    "shpl = shp.GetLayer()\n",
    "rs = gdal.GetDriverByName('GTiff')\n",
    "shpr = fshp[:-4]+\".tif\"\n",
    "rs = rs.Create(shpr, img.RasterXSize, img.RasterYSize, 1, gdal.GDT_Byte)\n",
    "rs.SetGeoTransform(img.GetGeoTransform())\n",
    "rs.SetProjection(img.GetProjectionRef())\n",
    "gdal.RasterizeLayer(rs, [1], shpl, options=[str(\"ATTRIBUTE=\" + attr)])\n",
    "rs = None\n",
    "\n",
    "# import rasterized shapefile\n",
    "spl = gdal.Open(shpr)\n",
    "splval= spl.ReadAsArray()\n",
    "\n",
    "# create image patches\n",
    "def savePatch(cla, idx, loc, path):\n",
    "    patchFileName = os.path.join(path, str(cla) + \"_\" + area + \"_\" + \\\n",
    "                                 str('{0:0'+str(len(str(maxPatchesPerClass)))+'d}').format(idx+1) + \".tif\")\n",
    "    gdal.Translate(patchFileName, fimg, format = 'GTiff', srcWin = [loc[1]-patchSize/2,\n",
    "                                                                    loc[0]-patchSize/2,\n",
    "                                                                    patchSize,\n",
    "                                                                    patchSize])\n",
    "\n",
    "disGrid = np.zeros((splval.shape[0], splval.shape[1]))\n",
    "disGrid[::sampleDistance, ::sampleDistance] = 1\n",
    "\n",
    "pixelsNum = np.zeros((numClasses))\n",
    "patchesNum = np.zeros((numClasses))\n",
    "\n",
    "for cla in np.unique(splval):\n",
    "    if cla > 0 and cla < 99:\n",
    "        xy = np.argwhere(splval == cla)\n",
    "        xy = xy[np.random.permutation(xy.shape[0])]\n",
    "        pixelsNum[cla-1] = xy.shape[0]\n",
    "        \n",
    "        xyList = []\n",
    "        maxCounter = 0\n",
    "        for loc in xy:\n",
    "            if disGrid[loc[0], loc[1]] == 1 and maxCounter < maxPatchesPerClass:\n",
    "                maxCounter += 1\n",
    "                patchesNum[cla-1] += 1\n",
    "                xyList.append(loc)\n",
    "        \n",
    "        split = int(patchesNum[cla-1] * trainTestRatio)\n",
    "        patchesTrain = xyList[:split]\n",
    "        patchesTest= xyList[split:]\n",
    "        \n",
    "        for idx, loc in enumerate(patchesTrain):\n",
    "            savePatch(cla, idx, loc, dptr)\n",
    "        for idx, loc in enumerate(patchesTest):\n",
    "            savePatch(cla, idx, loc, dpte)\n",
    "            \n",
    "print(\"pixel per class: \", pixelsNum)\n",
    "print(\"patTr per class: \", patchesNum * trainTestRatio)\n",
    "print(\"patTe per class: \", patchesNum * (1-trainTestRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NORMALIZE\n",
    "\n",
    "pTrain = os.listdir(dptr)\n",
    "pTest = os.listdir(dpte)\n",
    "\n",
    "patchesX = []\n",
    "for idx, f in enumerate(pTrain):\n",
    "    patchval = gdal.Open(os.path.join(dptr, f)).ReadAsArray()\n",
    "    patchval = np.moveaxis(patchval, 0, -1)\n",
    "    patchesX.append(patchval)\n",
    "for idx, f in enumerate(pTest):\n",
    "    patchval = gdal.Open(os.path.join(dpte, f)).ReadAsArray()\n",
    "    patchval = np.moveaxis(patchval, 0, -1)\n",
    "    patchesX.append(patchval)\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "z = np.array(patchesX, dtype=np.float32)\n",
    "for i in range(z.shape[3]):\n",
    "    means.append(np.mean(z[:, :, :, i]))\n",
    "    stds.append(np.std(z[:, :, :, i]) )\n",
    "    \n",
    "np.savetxt(os.path.join(dinp, \"means.txt\"), means)\n",
    "np.savetxt(os.path.join(dinp, \"stds.txt\"), stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TF MODEL SETUP\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, patchSize, patchSize, numChannels])\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "phaseTrain = tf.placeholder(tf.bool)\n",
    "\n",
    "def conv2d(x, W, b, strides=1, phase=0):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    x = tf.layers.batch_normalization(x, training=phase)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout, phaseTrain):\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'], phase=phaseTrain)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'], phase=phaseTrain)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, numChannels, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4096, 128])),\n",
    "    'out': tf.Variable(tf.random_normal([128, numClasses]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([128])),\n",
    "    'out': tf.Variable(tf.random_normal([numClasses]))\n",
    "}\n",
    "\n",
    "logits = conv_net(X, weights, biases, keep_prob, phaseTrain)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "\n",
    "# tensorboard\n",
    "tf.summary.image('input_x', X[:12, :, :, :3], 12) \n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "merge = tf.summary.merge_all()\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d_%H_%M\")\n",
    "train_writer = tf.summary.FileWriter(os.path.join(dtbl, \"train\", time))\n",
    "test_writer = tf.summary.FileWriter(os.path.join(dtbl, \"test\", time))\n",
    "\n",
    "# import statistics\n",
    "means = np.loadtxt(os.path.join(dinp, \"means.txt\"))\n",
    "stds = np.loadtxt(os.path.join(dinp, \"stds.txt\"))\n",
    "\n",
    "# import test patches\n",
    "pTest = os.listdir(dpte)\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for f in pTest:\n",
    "    pclass = int(f.split(\"_\")[0])\n",
    "    labtmp = np.zeros((numClasses))\n",
    "    labtmp[pclass-1] = 1\n",
    "    Y_test.append(labtmp)\n",
    "    \n",
    "    patchval = gdal.Open(os.path.join(dpte, f)).ReadAsArray()\n",
    "    patchval = np.array(patchval, dtype=np.float32)\n",
    "    for i in range(patchval.shape[0]):\n",
    "        patchval[i, :, :] = (patchval[i, :, :] - means[i]) / stds[i]\n",
    "    patchval = np.moveaxis(patchval, 0, -1)\n",
    "    X_test.append(patchval)\n",
    "\n",
    "# calc batch sizes\n",
    "pTrain = os.listdir(dptr)\n",
    "pTrain = [pTrain[i] for i in np.random.permutation(len(pTrain))]\n",
    "\n",
    "batches = []\n",
    "for i in range(0, len(pTrain), batchsize):\n",
    "    batches.append(i)\n",
    "if not batches[-1] == len(pTrain):\n",
    "    batches.append(len(pTrain))\n",
    "\n",
    "def getTrainBatch(b):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for f in pTrain[batches[b]:batches[b+1]]:\n",
    "        pclass = int(f.split(\"_\")[0])\n",
    "        labtmp = np.zeros((numClasses))\n",
    "        labtmp[pclass-1] = 1\n",
    "\n",
    "        patchval = gdal.Open(os.path.join(dptr, f)).ReadAsArray()\n",
    "        patchval = np.array(patchval, dtype=np.float32)\n",
    "        for i in range(patchval.shape[0]):\n",
    "            patchval[i, :, :] = (patchval[i, :, :] - means[i]) / stds[i]\n",
    "        \n",
    "        # augmentation\n",
    "        for r in range(0,4):\n",
    "            # rotate\n",
    "            patchValRotated = np.rot90(patchval, k=r, axes=(1, 2))\n",
    "            X_train.append(np.moveaxis(patchValRotated, 0, -1))\n",
    "            Y_train.append(labtmp)\n",
    "            # flip rotated \n",
    "            patchValRotated = np.fliplr(patchValRotated)\n",
    "            X_train.append(np.moveaxis(patchValRotated, 0, -1))\n",
    "            Y_train.append(labtmp)\n",
    "\n",
    "    return [X_train, Y_train]\n",
    "    \n",
    "# start training\n",
    "saver = tf.train.Saver()\n",
    "print(\"Epoch\".ljust(len(str(epochs))*2+4) + \"Train   Test    MBLoss\")\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1, epochs+1): \n",
    "        for b in range(len(batches)-1):\n",
    "            \n",
    "            X_train, Y_train = getTrainBatch(b)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X: X_train, Y: Y_train, keep_prob: dropout, phaseTrain: 1})\n",
    "\n",
    "        if epoch % display_step == 0 or epoch == 1:\n",
    "            summaryTr, lossTr, accTr = sess.run([merge, loss_op, accuracy],\n",
    "                                                feed_dict={X: X_train,\n",
    "                                                           Y: Y_train,\n",
    "                                                           keep_prob: 1.0,\n",
    "                                                           phaseTrain: 0})\n",
    "            \n",
    "            summaryTe, accTe = sess.run([merge, accuracy], feed_dict={X: X_test,\n",
    "                                                                      Y: Y_test,\n",
    "                                                                      keep_prob: 1.0,\n",
    "                                                                      phaseTrain: 0})\n",
    "\n",
    "            print(str('{0:0'+str(len(str(epochs)))+'d}').format(epoch) + \"/\" + str(epochs) + \\\n",
    "                  \"   {:.3f}\".format(accTr) + \\\n",
    "                  \"   {:.3f}\".format(accTe) + \\\n",
    "                  \"   {:.3f}\".format(lossTr))\n",
    "\n",
    "            train_writer.add_summary(summaryTr, epoch)\n",
    "            test_writer.add_summary(summaryTe, epoch)\n",
    "\n",
    "            save_path = saver.save(sess, fmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## VALIDATION\n",
    "\n",
    "# import statistics\n",
    "means = np.loadtxt(os.path.join(dinp, \"means.txt\"))\n",
    "stds = np.loadtxt(os.path.join(dinp, \"stds.txt\"))\n",
    "\n",
    "# import test patches\n",
    "pTest = os.listdir(dpte)\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Y_ref = np.zeros((len(pTest))).astype(int)\n",
    "\n",
    "for idx, f in enumerate(pTest):\n",
    "    pclass = int(f.split(\"_\")[0])    \n",
    "    labtmp = np.zeros((numClasses))\n",
    "    labtmp[pclass-1] = 1\n",
    "    Y_test.append(labtmp)\n",
    "    \n",
    "    Y_ref[idx] = pclass\n",
    "    \n",
    "    patchval = gdal.Open(os.path.join(dpte, f)).ReadAsArray()\n",
    "    patchval = np.array(patchval, dtype=np.float32)\n",
    "    for i in range(patchval.shape[0]):\n",
    "        patchval[i, :, :] = (patchval[i, :, :] - means[i]) / stds[i]\n",
    "    patchval = np.moveaxis(patchval, 0, -1)\n",
    "    X_test.append(patchval)\n",
    "\n",
    "# predict\n",
    "with tf.Session() as sess:  \n",
    "    tf.train.Saver().restore(sess, fmod)\n",
    "    \n",
    "    Y_pred = sess.run(prediction, feed_dict={X: X_test, Y: Y_test, keep_prob:1.0, phaseTrain: 0})\n",
    "    Y_pred = np.argmax(Y_pred, 1) + 1\n",
    "\n",
    "# accuracy matrix\n",
    "accMat = np.zeros((numClasses, numClasses))\n",
    "for i in range(Y_ref.shape[0]):\n",
    "    accMat[Y_pred[i]-1, Y_ref[i]-1] += 1\n",
    "print(accMat)\n",
    "\n",
    "# accuracies\n",
    "print(\"UA\", np.around(np.diagonal(accMat) / np.sum(accMat, 0), 2))\n",
    "print(\"PA\", np.around(np.diagonal(accMat) / np.sum(accMat, 1), 2))\n",
    "print(\"OA\", np.around(np.sum(np.diagonal(accMat)) / np.sum(accMat) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CLASSIFICATION\n",
    "\n",
    "# import\n",
    "imgc = gdal.Open(targ)\n",
    "img = imgc.ReadAsArray().astype(np.float32)\n",
    "\n",
    "# normalize\n",
    "for i in range(len(means)):\n",
    "    img[i, :, :] = (img[i, :, :] - means[i]) / stds[i]\n",
    "\n",
    "# classify\n",
    "cmap = np.zeros((img.shape[1], img.shape[2]))\n",
    "Ypatches = np.zeros((img.shape[2]-patchSize, numClasses))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, fmod)\n",
    "    \n",
    "    for x in range(0, img.shape[1]-patchSize):\n",
    "        Xpatches = []\n",
    "        for y in range(0, img.shape[2]-patchSize):\n",
    "            Xpatch = img[:, x:(x+patchSize), y:(y+patchSize)]\n",
    "            Xpatch = np.moveaxis(Xpatch, 0, -1)\n",
    "            Xpatches.append(Xpatch)\n",
    "            \n",
    "        pred = sess.run(prediction, feed_dict={X: Xpatches, Y: Ypatches, keep_prob:1.0, phaseTrain: 0})\n",
    "        cmap[x+int(patchSize/2), int(patchSize/2):int(patchSize/2)+pred.shape[0]] = np.argmax(pred, 1) + 1\n",
    "\n",
    "        sys.stdout.write('\\rProgress ' + str(\"{0:.2f}\".format(round(x / (img.shape[1]-patchSize)*100, 2))) + \"%\")\n",
    "        \n",
    "# plot\n",
    "plt.figure(figsize=(20,10))\n",
    "imgplot = plt.imshow(cmap)\n",
    "\n",
    "# export\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "df = driver.Create(targ[:-4]+\"_deep.tif\", imgc.RasterXSize, imgc.RasterYSize, 1, gdal.GDT_Byte)\n",
    "df.SetGeoTransform(imgc.GetGeoTransform())\n",
    "df.SetProjection(imgc.GetProjectionRef())\n",
    "df.GetRasterBand(1).WriteArray(cmap)\n",
    "df.FlushCache()\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## OLOFFSON ACC\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "e = accMat\n",
    "c, a = np.unique(cmap, return_counts=True)\n",
    "a = a[1:]\n",
    "\n",
    "img = gdal.Open(fimg)\n",
    "gt = img.GetGeoTransform()\n",
    "areaFac = gt[1] * -gt[5] / 10000\n",
    "\n",
    "p = (1.0 * e.transpose() * a / np.sum(a) / np.sum(e, 1)).transpose()\n",
    "\n",
    "print(\"AREA PROD\")\n",
    "PArea = areaFac * np.sum(p, 0) * np.sum(a)\n",
    "PAreaCI = areaFac * 1.96 * np.sum(a) * np.sqrt(np.sum((((((((1. * e.transpose() / np.sum(e, 1)).transpose()) * (1 - ((1. * e.transpose() / np.sum(e, 1)).transpose()))).transpose() / (np.sum(e, 1) - 1)).transpose()).transpose() * np.power(np.sum(p, 1), 2)).transpose()), 0))\n",
    "print(np.round(PArea, 2))\n",
    "print(np.round(PAreaCI, 2))\n",
    "\n",
    "print(\"UA\")\n",
    "ua = (np.diagonal(p).transpose() / np.sum(p, 1)).transpose()\n",
    "uaCI = 1.96 * np.sqrt(ua * (1 - ua) / (np.sum(e, 1) - 1))\n",
    "print(np.around(ua, 2))\n",
    "print(np.around(uaCI, 2))\n",
    "\n",
    "print(\"PA\")\n",
    "pa = np.diagonal(p) / np.sum(p, 0)\n",
    "N = []\n",
    "X = []\n",
    "for i in range(0, len(a)):\n",
    "    N.append(np.sum(a / np.sum(e, 1) * e[:, i]))\n",
    "    xa = np.array([x for j, x in enumerate(a) if j != i])\n",
    "    xb = np.array(e[[x for j, x in enumerate(np.arange(len(a))) if j != i], i])\n",
    "    xc = np.array([x for j, x in enumerate(np.sum(e, 1)) if j != i])\n",
    "    X.append(np.sum(xa ** 2 * xb / xc * (1 - xb / xc) / (xc - 1)))\n",
    "paCI = 1.96 * np.sqrt(1. / (np.array(N) ** 2) * (a ** 2 * (1 - pa) ** 2 * ua * (1 - ua) / (np.sum(e, 1) - 1) + pa ** 2 * X))\n",
    "print(np.around(pa, 2))\n",
    "print(np.around(paCI, 2))\n",
    "\n",
    "print(\"OA\")\n",
    "oa = np.sum(np.diagonal(p))\n",
    "oaCI = 1.96 * np.sqrt(np.sum(np.sum(p, 1) * np.sum(p, 1) * ua * (1 - ua) / (np.sum(e, 1) - 1)))\n",
    "print(np.around(oa, 2))\n",
    "print(np.around(oaCI, 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ten]",
   "language": "python",
   "name": "conda-env-ten-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
